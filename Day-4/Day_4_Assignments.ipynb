{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1\n",
        "You receive a raw support message written by a customer. Use an LLM API to extract the key fields and return a strict JSON object with these keys:\n",
        "\n",
        "customer_name (string or null)\n",
        "email (string or null)\n",
        "product (string or null)\n",
        "issue_type (one of: Billing, Login, Bug, Feature Request, Account, Other)\n",
        "urgency (one of: Low, Medium, High)\n",
        "summary (1–2 sentences)\n",
        "requested_action (string, what the customer wants)\n",
        "order_id (string or null)\n",
        "Sample input: “I’m Ravi Kumar (ravi.kumar22@gmail.com). I was charged twice for the Pro plan yesterday. My order id is ORD-88421. Please refund the extra charge asap. I need this fixed today because my manager is asking.”\n",
        "\n",
        "Sample output (JSON):\n",
        "\n",
        "{\n",
        "  \"customer_name\": \"Ravi Kumar\",\n",
        "  \"email\": \"ravi.kumar22@gmail.com\",\n",
        "  \"product\": \"Pro plan\",\n",
        "  \"issue_type\": \"Billing\",\n",
        "  \"urgency\": \"High\",\n",
        "  \"summary\": \"Customer reports being charged twice for the Pro plan and requests a refund for the duplicate charge. They need it resolved today due to managerial pressure.\",\n",
        "  \"requested_action\": \"Refund the extra charge and confirm the billing correction.\",\n",
        "  \"order_id\": \"ORD-88421\"\n",
        "}"
      ],
      "metadata": {
        "id": "uUzNjTx9m4mj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G-LscFC-muSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78887228-27f6-4009-ca9b-e1f626b169c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------LLM Response----------------\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"summary\": \"Customer Ravi Kumar reports being charged twice for the Pro plan (Order ID: ORD-88421) and requests an immediate refund today due to internal management pressure.\",\n",
            "  \"product\": \"Pro plan\",\n",
            "  \"key_points\": {\n",
            "    \"customer_name\": \"Ravi Kumar\",\n",
            "    \"email\": \"ravi.kumar22@gmail.com\",\n",
            "    \"order_id\": \"ORD-88421\",\n",
            "    \"issue\": \"Duplicate billing (charged twice)\",\n",
            "    \"transaction_date\": \"Yesterday\",\n",
            "    \"requested_action\": \"Refund the extra charge\",\n",
            "    \"urgency_level\": \"High (Needs resolution today)\",\n",
            "    \"reason_for_urgency\": \"Manager is inquiring about the charge\"\n",
            "  }\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "#Problem-1\n",
        "from google.colab import userdata\n",
        "\n",
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "sample_input = \"“I’m Ravi Kumar (ravi.kumar22@gmail.com). I was charged twice for the Pro plan yesterday. My order id is ORD-88421. Please refund the extra charge asap. I need this fixed today because my manager is asking.”\"\n",
        "prompt = f\"Extract the Key Points in {sample_input} in strict JSON object. Also include the 1-2 line summary,product for{sample_input} in same JSON object,arrrange them in proper manner\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    contents=prompt,\n",
        ")\n",
        "print(\"\\n--------------LLM Response----------------\\n\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2\n",
        "You are given rough bullet points (sometimes broken grammar). Use an LLM API to generate a polished professional email. Return a strict JSON object with these keys:\n",
        "\n",
        "subject (string)\n",
        "email_body (string, must include greeting + clear paragraphs + sign-off)\n",
        "Sample input (bullets):\n",
        "\n",
        "need follow up on invoice\n",
        "last week sent, no reply\n",
        "ask if any issue with payment\n",
        "mention due date Jan 5\n",
        "offer help if they need anything\n",
        "sign as “Chandra Sekhar”\n",
        "Sample output (JSON):\n",
        "\n",
        "{\n",
        "  \"subject\": \"Follow-up on Invoice and Payment Status\",\n",
        "  \"email_body\": \"Hi there,\\n\\nI’m following up regarding the invoice I shared last week. I wanted to check if you had a chance to review it and whether there are any issues from your side with the payment process.\\n\\nAs a quick reminder, the due date is January 5. If you need any clarification or assistance, I’m happy to help.\\n\\nThanks,\\nChandra Sekhar\"\n",
        "}"
      ],
      "metadata": {
        "id": "4N5OUj2Fq5_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem-2\n",
        "sample_bullets = ['need follow up on invoice',\n",
        "                  'last week sent, no reply',\n",
        "                  'ask if any issue with payment',\n",
        "                  'mention due date Jan 5',\n",
        "                  'offer help if they need anything',\n",
        "                  'sign as “Chandra Sekhar”'\n",
        "                  ]\n",
        "prompt = f\"Generate a Polished Professional Email for bullets in {sample_bullets} Produce a Strict JSON Object with 'subject,email_body'\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    contents=prompt,\n",
        ")\n",
        "print(\"\\n--------------LLM Response----------------\\n\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8Z56DIrrAVf",
        "outputId": "2dd0808a-89f5-4a1c-d731-4b88addcf971"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------LLM Response----------------\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"subject\": \"Follow-up: Invoice Status - Due January 5\",\n",
            "  \"email_body\": \"Dear [Recipient Name],\\n\\nI hope you are having a productive week. \\n\\nI am writing to follow up on the invoice I sent last week, as I haven't received a response regarding its receipt. Could you please let me know if there are any issues with the payment or if you require any further documentation from my side?\\n\\nAs a reminder, the payment is due by January 5th. Please let me know if you need any assistance or if there is anything I can do to help facilitate the process.\\n\\nBest regards,\\n\\nChandra Sekhar\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3\n",
        "You receive an incoming message to a company inbox. Use an LLM API to classify and route the message. Return a strict JSON object with these keys:\n",
        "\n",
        "category (one of: Billing, Technical, Sales, Cancellation, Feedback, Other)\n",
        "\n",
        "priority (one of: P0, P1, P2)\n",
        "\n",
        "P0: critical / urgent / money loss / service unusable\n",
        "P1: major issue but workaround exists\n",
        "P2: minor issue / general request\n",
        "team (one of: Billing Team, Tech Support, Sales Team, Retention, General)\n",
        "\n",
        "next_action (string, one clear instruction)\n",
        "\n",
        "clarifying_question (string or null)\n",
        "\n",
        "Sample input: “Hi, after yesterday’s update the dashboard is blank on Safari. Chrome works. I have a client demo in 2 hours—please help fast.”\n",
        "\n",
        "Sample output (JSON):\n",
        "\n",
        "{\n",
        "  \"category\": \"Technical\",\n",
        "  \"priority\": \"P0\",\n",
        "  \"team\": \"Tech Support\",\n",
        "  \"next_action\": \"Create an urgent bug ticket for Safari dashboard blank screen; request Safari version and console errors.\",\n",
        "  \"clarifying_question\": \"Can you share your Safari version and a screenshot (or console errors) from the blank dashboard?\"\n",
        "}"
      ],
      "metadata": {
        "id": "oUUDi2HCtOzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem-3\n",
        "sample_msg = \"Hi, after yesterday’s update the dashboard is blank on Safari. Chrome works. I have a client demo in 2 hours—please help fast.\"\n",
        "prompt = f\"consider {sample_msg} is a incoming message to a company inbox. Now classify and route {sample_msg} in strict JSON object with keys: 1.Category 2.Priority(P0=Critical,P1=Major,P3=Minor) 3.Team 4. Next_Action 5. Clarifying Question\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    contents=prompt,\n",
        ")\n",
        "print(\"\\n--------------LLM Response----------------\\n\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQE3ZS5ItfJ8",
        "outputId": "237c6a46-d641-4977-9e27-1326279ffde1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------LLM Response----------------\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"Category\": \"Technical Bug Report / Regression\",\n",
            "  \"Priority\": \"P0\",\n",
            "  \"Team\": \"Frontend Engineering / Product Support\",\n",
            "  \"Next_Action\": \"Immediately escalate to the on-call engineer to investigate Safari-specific deployment errors and provide a workaround or hotfix before the 2-hour deadline.\",\n",
            "  \"Clarifying Question\": \"Which version of Safari and what operating system (macOS/iOS) are you currently using?\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4\n",
        "You are given a context text (policy/doc snippet) and a user question. Use an LLM API to answer ONLY using the provided context. Return a strict JSON object with these keys:\n",
        "\n",
        "answer (string; if not present in context, output exactly: \"Not found in provided context.\")\n",
        "evidence (array of exact sentences copied from the context that support the answer)\n",
        "confidence (one of: Low, Medium, High)\n",
        "Sample input:\n",
        "\n",
        "Context text: “Refund Policy:\n",
        "\n",
        "Full refunds are available within 7 days of purchase.\n",
        "After 7 days and within 30 days, refunds are prorated based on usage.\n",
        "No refunds are issued after 30 days.\n",
        "Refunds are processed within 5–7 business days.”\n",
        "Question: “I purchased 10 days ago. Can I get a full refund?”\n",
        "\n",
        "Sample output (JSON):\n",
        "\n",
        "{\n",
        "  \"answer\": \"No. Since the purchase was 10 days ago (within 30 days but after 7 days), the refund would be prorated based on usage, not a full refund.\",\n",
        "  \"evidence\": [\n",
        "    \"After 7 days and within 30 days, refunds are prorated based on usage.\"\n",
        "  ],\n",
        "  \"confidence\": \"High\"\n",
        "}"
      ],
      "metadata": {
        "id": "OmviLcB-vhNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem-4\n",
        "context_text = \"\"\"\n",
        "                  Refund Policy :\n",
        "                  1.Full refunds are available within 7 days of purchase.\n",
        "                  2.After 7 days and within 30 days, refunds are prorated based on usage.\n",
        "                  3.No refunds are issued after 30 days.\n",
        "                  4.Refunds are processed within 5–7 business days\n",
        "                  \"\"\"\n",
        "question = input(\"Enter Your Query Here: \")\n",
        "keys = [\n",
        "          'Answer',\n",
        "          'Evidence',\n",
        "          'Confidence(Low,Medium,High,)'\n",
        "       ]\n",
        "prompt = f\"\"\"\n",
        "Memorize Refund Policy as {context_text} and\n",
        "answer the {question} in strict JSON object with keys: {keys}\n",
        "if {question} doesn't matches {context_text} simply print 'Not found in provided context'\n",
        "\"\"\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    contents=prompt,\n",
        ")\n",
        "print(\"\\n--------------LLM Response----------------\\n\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyKHc8sawZrg",
        "outputId": "6883e6cc-27b1-451f-bea8-080d6e8a7465"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Your Query Here: I purchased 10 days ago. Can I get a full refund?\n",
            "\n",
            "--------------LLM Response----------------\n",
            "\n",
            "{\n",
            "  \"Answer\": \"No, you cannot get a full refund. Since you purchased 10 days ago, you are outside the 7-day window for a full refund and fall under the prorated refund policy.\",\n",
            "  \"Evidence\": \"1.Full refunds are available within 7 days of purchase. 2.After 7 days and within 30 days, refunds are prorated based on usage.\",\n",
            "  \"Confidence(Low,Medium,High,)\": \"High\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5 (Simple 2-Step Chain)\n",
        "You are building a mini content pipeline using an LLM API with two chained steps:\n",
        "\n",
        "Step 1: From the given topic + audience, generate a clear outline. Step 2: Using only that outline, generate the final content.\n",
        "\n",
        "Return a strict JSON object with these keys:\n",
        "\n",
        "outline (array of section titles)\n",
        "final_content (string, must follow the outline order)\n",
        "quality_check (string, 1–2 lines confirming the content matches the outline)\n",
        "Sample input:\n",
        "\n",
        "topic: “Async/Await in JavaScript”\n",
        "audience: “Beginners who know callbacks”\n",
        "length: “~250 words”\n",
        "tone: “Simple and friendly”\n",
        "Sample output (JSON):\n",
        "\n",
        "{\n",
        "  \"outline\": [\n",
        "    \"What problem async/await solves\",\n",
        "    \"The basic syntax (with a tiny example)\",\n",
        "    \"What happens when there is an error (try/catch)\",\n",
        "    \"Common beginner mistakes\",\n",
        "    \"Quick recap\"\n",
        "  ],\n",
        "  \"final_content\": \"Async/await helps you write asynchronous code that looks and feels like normal step-by-step code. It mainly solves the problem of messy nested callbacks or long `.then()` chains that are hard to read...\\n\\nThe basic idea is: you mark a function as `async`, and inside it you can use `await` before a promise to pause until it finishes...\\n\\nWhen something goes wrong (like a failed API call), you handle it using `try/catch`...\\n\\nBeginners often forget that `await` works only inside an `async` function, or they assume `await` makes code run faster (it doesn’t—it just makes it easier to read)...\\n\\nRecap: use `async` to enable `await`, use `await` to wait for promises, and wrap risky code in `try/catch` for errors.\",\n",
        "  \"quality_check\": \"Content follows the outline in the same order and stays beginner-friendly with a simple explanation and common mistakes.\"\n",
        "}"
      ],
      "metadata": {
        "id": "VJR-1ynczxX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem-5\n",
        "from google.colab import userdata\n",
        "\n",
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=userdata.get('Assignments'))\n",
        "topic = \"Async/Await in JavaScript\"\n",
        "audience = \"Beginners who know callbacks\"\n",
        "length = \"Approx. 250 words\"\n",
        "tone = \"Simple & Friendly\"\n",
        "keys = [\n",
        "    'Outline''(array of section title)',\n",
        "    'Final_Content''(string, must follow the outline order)',\n",
        "    'Quality_Check''(1-2 lines confirming content matches outline)'\n",
        "]\n",
        "prompt = f\"\"\"\n",
        "              from {topic} + {audience} generate a clear outline.\n",
        "              only using the generated outline generate the Final Content of size:{length}\n",
        "              generate a JSON object with keys: {keys}\n",
        "         \"\"\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    contents=prompt,\n",
        ")\n",
        "print(\"\\n--------------LLM Response----------------\\n\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRn7tJu30DaL",
        "outputId": "c3747804-b050-4b76-ef81-abe7543b601c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------LLM Response----------------\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"Outline\": [\n",
            "    \"Introduction: Moving Beyond Callback Hell\",\n",
            "    \"Defining Async/Await as Syntactic Sugar\",\n",
            "    \"The Function of the 'async' Keyword\",\n",
            "    \"The Power of the 'await' Keyword\",\n",
            "    \"Simplified Error Handling with Try/Catch\",\n",
            "    \"Conclusion: Clean and Maintainable Code\"\n",
            "  ],\n",
            "  \"Final_Content\": \"If you have ever dealt with deeply nested callbacks, you know the frustration often called \\\"Callback Hell.\\\" While callbacks are the functional foundation of asynchronous JavaScript, managing multiple dependent operations can quickly lead to messy, unreadable, pyramid-shaped code. This is where Async/Await comes in—a modern syntax introduced in ES2017 designed to make asynchronous operations look and behave more like traditional synchronous ones. Syntactically, async and await are known as \\\"syntactic sugar\\\" built on top of Promises. They provide a much cleaner way to handle non-blocking tasks without the overhead of complex chains. To use this feature, you start with the 'async' keyword. When you label a function as async, it automatically ensures that the function returns a Promise, wrapping any returned value accordingly. This sets the stage for the 'await' keyword, which is valid only inside these specific functions. The 'await' keyword is the real power of this syntax. It pauses the execution of the function line-by-line until a Promise is settled. Instead of relying on .then() callbacks, you can assign the result of a fetch or database call directly to a variable. This linear flow makes the logic drastically easier for developers to read and debug. Furthermore, error handling becomes significantly more intuitive. Instead of checking for an error parameter inside every callback, you can wrap your code in standard try/catch blocks. This allows you to handle errors from multiple asynchronous steps in one centralized location. By moving away from callbacks, you achieve code that is cleaner, safer, and far more professional.\",\n",
            "  \"Quality_Check\": \"The content strictly follows the six-point outline provided, transitioning logically from callback issues to modern syntax and error handling. The word count is approximately 245 words, meeting the target length.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    }
  ]
}